{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport os\nimport pickle\nfrom tqdm.notebook import tqdm\n\nfrom tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\nfrom tensorflow.keras.preprocessing.image import img_to_array, load_img\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_dir='/kaggle/input/flickr8k'\nworking_dir='/kaggle/working'\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=VGG16()\nmodel=Model(inputs=model.inputs,outputs=model.layers[-2].output)\nprint(model.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#make an empty dictionary for storing the features\nfeatures={}\n\n#make a path to the pictures\ndirectory=os.path.join(base_dir,'Images')\n\n#extracting features from images\n#for every image in teh directory do the following\nfor img in os.listdir(directory):\n    #make the full image path and store it ina seperate variable\n    img_path=directory+\"/\"+img\n    #load the image into a different variable and make sure the image isze is all the same\n    image=load_img(img_path,target_size=(224,224))\n    #converting pixel values into numpy aray and storing\n    image=img_to_array(image)\n    #reshaping the image,i.e making sure that it is the same shape as the original image\n    image=image.reshape(1,image.shape[0],image.shape[1],image.shape[2])\n    #preprocess the image\n    image=preprocess_input(image)\n    feature=model.predict(image,verbose=1)\n    #get the image id\n    image_id=img.split('.')[0]\n    features[image_id]=feature #stores a key value pair\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#use pcikel to write and store the features extracted into a binary file\npickle.dump(features,open(os.path.join(working_dir,'features.pkl'),'wb'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#seeing the contents of the featuresd\nwith open(os.path.join(working_dir,'features.pkl'),'rb') as f:\n    loaded_features=pickle.load(f)\n    \nfor key,value in list(loaded_features.items())[:5]:\n    #print( value)\n    print(value.shape)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#reading the captions that are present in the data\nwith open(os.path.join(base_dir,'captions.txt'),'r') as ff:\n    next(ff)\n    captions=ff.read()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#total number of captions\nlength=captions.split('\\n')\nprint(len(length))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(captions.split('\\n')[0])\nfirst=captions.split('\\n')[0]\ntokens=first.split(',')\nprint(len(tokens))\nprint(tokens[0].split('.')[0])\nprint(tokens[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"caption_map={}\n\nfor line in captions.split('\\n'):\n    if len(line)<2:\n        continue\n    #now you have each line  as id,caption pair- get the id and caption seperated\n    token=line.split(',')\n    caption_id=token[0]\n    caption=token[1]\n    #get rid of the extension for the id\n    caption_id=caption_id.split('.')[0]\n    \n    #check if the id is in the dictionary otherwise create a new list w the id as the key\n    if caption_id not in caption_map:\n        caption_map[caption_id]=[]\n    #add the cpation to that list created\n    caption_map[caption_id].append(caption)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(caption_map)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#crate a class to preprocess the text \ndef clean(caption_map):\n    for key,captions in caption_map.items():\n        #the data is in the format such tha the key is one value and the caption is a list of sentences(5 in this dataset)\n        #you need to take each sentence and pre process it \n        for i in range(len(captions)):\n            caption=captions[i]#now you take one caption at a time\n            caption=caption.lower()\n            caption=caption.replace('[^A-Za-z]','')\n            #get rid of the excess white spaces as well\n            import re\n            caption=re.sub('\\s+',' ',caption)\n            #add the starting and ending token to find out the starting and ending\n            caption= \"startseq \"+\"  \".join([word for word in caption.split() if len(word)>1])+\" \"+\"endseq\"\n            captions[i]=caption","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"caption_map['1000268201_693b08cb0e']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#after cleaning data\nclean(caption_map)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"caption_map['1000268201_693b08cb0e']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#put all the captions in one large list\nall_captions=[]\nfor key in caption_map:\n    for caption in caption_map[key]:\n        all_captions.append(caption)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(all_captions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_captions[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tokenizing the text\ntokenizer=Tokenizer()\ntokenizer.fit_on_texts(all_captions)\nvocab_size=len(tokenizer.word_index)+1\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(vocab_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_len=max(len(caption.split()) for caption in all_captions)\nprint(max_len)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train test and split the dataset\n#store all the keys in a list\n\n#now you are justting the getting the keys for training, testing and splitting\nimage_ids=list(caption_map.keys())\nsplit=int(len(image_ids)*0.9)\ntrain=image_ids[:split]\ntest=image_ids[split:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def data_generator(data_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size):\n    # loop over images\n    X1, X2, y = list(), list(), list()\n    n = 0\n    while 1:\n        for key in data_keys:\n            n += 1\n            captions = mapping[key]\n            # process each caption\n            for caption in captions:\n                # encode the sequence\n                seq = tokenizer.texts_to_sequences([caption])[0]\n                # split the sequence into X, y pairs\n                for i in range(1, len(seq)):\n                    # split into input and output pairs\n                    in_seq, out_seq = seq[:i], seq[i]\n                    # pad input sequence\n                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n                    # encode output sequence\n                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n                    \n                    # store the sequences\n                    X1.append(features[key][0])\n                    X2.append(in_seq)\n                    y.append(out_seq)\n            if n == batch_size:\n                X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n                yield [X1, X2], y\n                X1, X2, y = list(), list(), list()\n                n = 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#calculating number of features\ncount=0\nfor keys in features.keys():\n    count+=1\n    print(keys)\n    if count>5:\n        break\nlength=len(features[\"3226254560_2f8ac147ea\"])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(features[\"3226254560_2f8ac147ea\"])\narr=np.array(features[\"3226254560_2f8ac147ea\"])\nprint(arr.shape) # it implies you have 4096 features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Creation\n","metadata":{}},{"cell_type":"code","source":"#you cannot use the normal sequential model that use normally, bc here you need multiple inputs so the layers cannot be \n#stacked ontop of eachother\n\n#use funtional API\n#image feature data\nfrom tensorflow.keras.layers import Input,Dense,Dropout,Embedding,LSTM,add\ninputs1=Input(shape=(4096,))\nfe1=Dropout(0.4)(inputs1)\nfe2=Dense(256,activation='relu')(fe1)\n\n\n#text data input\ninput2=Input(shape=(max_len,))\nse1=Embedding(vocab_size,256,mask_zero=True)(input2)\nse2=Dropout(0.4)(se1)\nse3=LSTM(256)(se2)\n\n\n#buildiing the decoder\n#combine both the models\ndecoder1=add([fe2,se3])\ndecoder2 = Dense(256, activation='relu')(decoder1)\noutputs = Dense(vocab_size, activation='softmax')(decoder2)\nmodel = Model(inputs=[inputs1, input2], outputs=outputs)\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\n# plot the model\nfrom tensorflow.keras.utils import plot_model\nplot_model(model, show_shapes=True)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#training the model\nbatch_size=32\nsteps=len(train)//32. #automatically returns floor so you dont have to use int explicitly\nepochs=20\n#generator = data_generator(train, caption_map, features, tokenizer, max_len, vocab_size, batch_size)\n#model.fit(generator, epochs=20, steps_per_epoch=steps, verbose=1)\n\nfor i in range(epochs):\n\n    # create data generator\n    generator = data_generator(train, caption_map, features, tokenizer, max_len, vocab_size, batch_size)\n    # fit for one epoch\n    model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#SAVING THE MODEL\nmodel.save(working_dir+'/best_model.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Funtion to turn index into words\n","metadata":{}},{"cell_type":"code","source":"#using the index find the word and return it\ndef idx_to_word(key,tokenizer):\n    for word,index in tokenizer.word_index.items():\n        if key==index:\n            return word\n            \n    return None\n\n        \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#a funtion to make everything work from the input\ndef predict_caption(model, image, tokenizer, max_len):#max len is the maximum length of the caption, so now you are assuming that the new caption cannot exceed the limit for this, so you will iterate that many ties\n    caption='startseq'\n    for i in range(max_len):\n        #you need to create a sequence and pad it, but which one? i think you can always start off\n        #you need to find the index of the word and then the word\n        #word index will be found by the model\n        sequence=tokenizer.texts_to_sequences([caption])[0]\n        seq=pad_sequences([sequence],maxlen=max_len)\n        y=model.predict([image,seq],verbose=0) #initially it will be startseq, the model will return the probabilities of all the possible words , you need to pick the one w the highest prob\n        y=np.argmax(y)\n        \n        word=idx_to_word(y,tokenizer)\n        if word is None:\n            break\n        caption+=\" \"+word\n        if word=='endseq':\n            break\n    return caption\n\n\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.translate.bleu_score import corpus_bleu\n# validate with test data\nactual, predicted = list(), list()\n\nfor key in test:\n    # get actual caption\n    captions = caption_map[key]\n    # predict the caption for image\n    y_pred = predict_caption(model, features[key], tokenizer, max_len) \n    # split into words\n    actual_captions = [caption.split() for caption in captions]\n    y_pred = y_pred.split()\n    # append to the list\n    actual.append(actual_captions)\n    predicted.append(y_pred)\n    \n# calcuate BLEU score\nprint(\"BLEU-1: %f\" % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\nprint(\"BLEU-2: %f\" % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nimport matplotlib.pyplot as plt\ndef generate_caption(image_name):\n    # load the image\n    # image_name = \"1001773457_577c3a7d70.jpg\"\n    image_id = image_name.split('.')[0]\n    img_path = os.path.join(base_dir, \"Images\", image_name)\n    image = Image.open(img_path)\n    captions = caption_map[image_id]\n    print('---------------------Actual---------------------')\n    for caption in captions:\n        print(caption)\n    # predict the caption\n    y_pred = predict_caption(model, features[image_id], tokenizer, max_len)\n    print('--------------------Predicted--------------------')\n    print(y_pred)\n    plt.imshow(image)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate_caption(\"1001773457_577c3a7d70.jpg\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate_caption(\"1002674143_1b742ab4b8.jpg\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate_caption(\"101669240_b2d3e7f17b.jpg\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}